{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://massstreetuniversity.com/\"><img src=\"https://tutorials.massstreetuniversity.com/images/logo.png\" alt=\"School Logo\"></a><br><br><br>\n",
    "<a href=\"https://tutorials.massstreetuniversity.com/python/\">Tutorial Home</a> | <a href=\"https://tutorials.massstreetuniversity.com/python/solutions/32-combine-csvs-into-one-file.html\">Previous</a> | <a href=\"https://tutorials.massstreetuniversity.com/python/solutions/34-efficient-disk-write.html\">Next</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Lesson 33. Load Large CSVs Into Data Warehouse Staging Tables</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BULK INSERT is a nice SQL statement to load large amounts of data from a flat file into a database table. The problem with BULK INSERT is that the schema of the file and the schema of the table have to be the same. That is problematic when the table you are loading has audit columns.\n",
    "\n",
    "The solution is to create a view based on the table that only has the columns that you want to load. You name the view and the table the same. The table should be in a schema. The view should be in the dbo schema. That way, you can reference both with the same name, and SQL Server knows when you are referring to the view or the table.\n",
    "\n",
    "When you write your BULK INSERT statement, you reference the view. That will load the actual table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Examples</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Example #1: Load A Large CSV</strong>\n",
    "\n",
    "In order to run this example, you will need to set up SQL Server by creating the staging table and the view.\n",
    "\n",
    "In the SQL directory of the source code directory SQL\\33 Load Large CSVs Into Staging, you will find two files sequentially named. Open them in SSMS and run them in the order specified by their file name. First the table, then the view.\n",
    "\n",
    "Now that we are good to go. We are going to take the large Eurostat data that we compiled, and shoehorn it into a table.\n",
    "\n",
    "In this example, I am going to introduce some feedback mechanisms in the form of timestamps. You can use these to understand how long the processes is taking and where the process is in the execution.\n",
    "\n",
    "Indexes slow the load of database tables. For small amounts of data, that is no big deal. When loading large files, you need to drop any indexes first. Once the file is loaded, you then rebuild any indexes that you dropped. \n",
    "\n",
    "This, of course, is a long running process. However, it is much faster than other methods.\n",
    "\n",
    "<strong>Script Performance</strong>\n",
    "\n",
    "This is not a toy example and it is entirely possible you could choke your machine. On my box, the script took about 30 minutes to load the data. My machine specs are below.\n",
    "\n",
    "Processor: Intel® Core™ i5-4590 CPU @ 3.30GHz<br>\n",
    "RAM: 16.0 GB<br>\n",
    "System type: 64-bit Operating System, x64-based processor<br>\n",
    "\n",
    "If your system has less RAM, expect the process to take longer. If your system performance is significantly less than what is stated above, do not attempt to run this script with the sample file. Instead, cope a file from the FileLoopExample folder and use that instead. Either rename the file or change the value of the file_name variable.\n",
    "\n",
    "<strong>Troubleshooting</strong>\n",
    "\n",
    "Before you run this example, you may want to review <a href=\"https://tutorials.massstreetuniversity.com/transact-sql/solutions/troubleshoot-queries.html\">Lesson 56. Troubleshooting Long Running Queries</a> from the Practical T-SQL Pocket Guide For Beginners.\n",
    "\n",
    "<strong>Viewing Results</strong>\n",
    "\n",
    "When the process has completed, you can view the results by running the SQL queries found after the Python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to SQL Server database 16:29:47\n",
      "Preparing database for update 16:29:47\n",
      "Begin processing EurostatDataCombined.csv. 16:29:47\n",
      "Updating staging\n",
      "Processing file EurostatDataCombined.csv complete. 17:01:56\n",
      "Complete: Processing Data 17:01:56\n"
     ]
    }
   ],
   "source": [
    "#import what you need and set up your file variables.\n",
    "import os\n",
    "import pyodbc as db\n",
    "import time\n",
    "\n",
    "script_dir = os.getcwd()\n",
    "data_directory = 'data\\\\'\n",
    "source_directory = 'CombineCSVExample\\\\'\n",
    "file_name = 'EurostatDataCombined.csv'\n",
    "source_path = os.path.join(script_dir, data_directory,source_directory, file_name)\n",
    "\n",
    "#Build SQL Statements\n",
    "drop_index_sql = 'ALTER TABLE [euro].[EurostatData] DROP CONSTRAINT [PK_EurostatData] WITH ( ONLINE = OFF )'\n",
    "\n",
    "add_index_sql = 'ALTER TABLE [euro].[EurostatData] ADD  CONSTRAINT [PK_EurostatData] PRIMARY KEY CLUSTERED'\n",
    "add_index_sql = add_index_sql + '([ETLKey] ASC) WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF,' \n",
    "add_index_sql = add_index_sql + 'SORT_IN_TEMPDB = OFF, IGNORE_DUP_KEY = OFF, ONLINE = OFF, ALLOW_ROW_LOCKS = ON,'\n",
    "add_index_sql = add_index_sql + 'ALLOW_PAGE_LOCKS = ON) ON [PRIMARY]'\n",
    "\n",
    "sql = \"BULK INSERT EurostatData FROM '\" + source_path + \"' WITH (FIELDTERMINATOR = '|', ROWTERMINATOR = '0x0a', FIRSTROW = 2, TABLOCK, BATCHSIZE = 100000)\"\n",
    "\n",
    "#Set up the connection.\n",
    "print('Connecting to SQL Server database' + time.strftime(' %H:%M:%S'))\n",
    "connection_string = 'DSN=ETL;'\n",
    "conn = db.connect(connection_string)\n",
    "print('Preparing database for update' + time.strftime(' %H:%M:%S'))\n",
    "csr = conn.cursor()\n",
    "\n",
    "\n",
    "#now let's load the file\n",
    "print('Begin processing {}.'.format(file_name) + time.strftime(' %H:%M:%S'))\n",
    "csr.execute('TRUNCATE TABLE euro.EurostatData')\n",
    "csr.execute(drop_index_sql)\n",
    "print('Updating staging')\n",
    "csr.execute(sql)\n",
    "csr.execute(add_index_sql)\n",
    "print('Processing file {} complete.'.format(file_name) + time.strftime(' %H:%M:%S'))\n",
    "conn.commit()\n",
    "csr.close()\n",
    "conn.close()\n",
    "\n",
    "print('Complete: Processing Data'  + time.strftime(' %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE ODS\n",
    "\n",
    "SELECT COUNT(*) FROM euro.EurostatData --31,599,999\n",
    "\n",
    "SELECT TOP 100 * FROM euro.EurostatData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2020, Mass Street Analytics, LLC. All Rights Reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
